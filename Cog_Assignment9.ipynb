{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPB5ElJLGI1bvkqN2nZsQDc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saumya0330/Saumya-USC420/blob/main/Cog_Assignment9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lab Assignment 9\n",
        "\n",
        "UCS420 Cognitive Computing\n",
        "\n",
        "Assignment Title: NLP using Python"
      ],
      "metadata": {
        "id": "f_L4TuGY9mmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuation.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribution (excluding stopwords)."
      ],
      "metadata": {
        "id": "GOHaBFKN0IxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text =\"Hello! How are you?\"\n",
        "#converting to lowercase\n",
        "text_lower=text.lower()\n",
        "print(text_lower)\n",
        "\n",
        "#various methods to remove punctuation\n",
        "#1.1\n",
        "rm1= re.sub(r'[^\\w\\s]','',text_lower)\n",
        "print(rm1)\n",
        "\n",
        "#1.2\n",
        "rm2=text_lower.translate(str.maketrans('','',string.punctuation))\n",
        "print(rm2)\n",
        "\n",
        "#1.3\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer=RegexpTokenizer(r'\\w+')\n",
        "rm3=tokenizer.tokenize(text_lower)\n",
        "print(rm3)\n",
        "\n",
        "#1.4\n",
        "tokens=word_tokenize(text_lower)\n",
        "rm4=[word for word in tokens if word not in string.punctuation]\n",
        "print(rm4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJy7NABDSPHI",
        "outputId": "a45ac172-cfb0-4590-f740-0258d44c0654"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello! how are you?\n",
            "hello how are you\n",
            "hello how are you\n",
            "['hello', 'how', 'are', 'you']\n",
            "['hello', 'how', 'are', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the text into words and sentences.\n",
        "\n",
        "text2 = \"Painting is an art form that brings colors, emotions, and stories to life on a canvas. Whether capturing the beauty of nature or exploring abstract ideas, it transforms imagination into visual expression, leaving a timeless impact!\"\n",
        "\n",
        "sentences=sent_tokenize(text2)\n",
        "print(sentences)\n",
        "words = word_tokenize(text2)\n",
        "print(words)\n",
        "\n",
        "clean_text = [word for word in words if word not in string.punctuation]\n",
        "print(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2VN3dzaPlQ",
        "outputId": "53cf1dc5-747e-431d-fd3d-37e0268ac27d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Painting is an art form that brings colors, emotions, and stories to life on a canvas.', 'Whether capturing the beauty of nature or exploring abstract ideas, it transforms imagination into visual expression, leaving a timeless impact!']\n",
            "['Painting', 'is', 'an', 'art', 'form', 'that', 'brings', 'colors', ',', 'emotions', ',', 'and', 'stories', 'to', 'life', 'on', 'a', 'canvas', '.', 'Whether', 'capturing', 'the', 'beauty', 'of', 'nature', 'or', 'exploring', 'abstract', 'ideas', ',', 'it', 'transforms', 'imagination', 'into', 'visual', 'expression', ',', 'leaving', 'a', 'timeless', 'impact', '!']\n",
            "['Painting', 'is', 'an', 'art', 'form', 'that', 'brings', 'colors', 'emotions', 'and', 'stories', 'to', 'life', 'on', 'a', 'canvas', 'Whether', 'capturing', 'the', 'beauty', 'of', 'nature', 'or', 'exploring', 'abstract', 'ideas', 'it', 'transforms', 'imagination', 'into', 'visual', 'expression', 'leaving', 'a', 'timeless', 'impact']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove stopwords (using NLTK's stopwords list).\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in clean_text if word not in stop_words]\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzx7LtOaa7A-",
        "outputId": "4e6ad1d7-672d-4c6f-ffa9-3b3392e68e64"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Painting', 'art', 'form', 'brings', 'colors', 'emotions', 'stories', 'life', 'canvas', 'Whether', 'capturing', 'beauty', 'nature', 'exploring', 'abstract', 'ideas', 'transforms', 'imagination', 'visual', 'expression', 'leaving', 'timeless', 'impact']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Display word frequency distribution (excluding stopwords).\n",
        "from collections import Counter\n",
        "\n",
        "word_freq = Counter(filtered_words)\n",
        "#print(word_freq)\n",
        "print('word frquency distribution:')\n",
        "for word, freq in word_freq.items():\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRMnxku9ceTy",
        "outputId": "7fc16461-02b9-4279-b9ce-9a3e777694f5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word frquency distribution:\n",
            "Painting: 1\n",
            "art: 1\n",
            "form: 1\n",
            "brings: 1\n",
            "colors: 1\n",
            "emotions: 1\n",
            "stories: 1\n",
            "life: 1\n",
            "canvas: 1\n",
            "Whether: 1\n",
            "capturing: 1\n",
            "beauty: 1\n",
            "nature: 1\n",
            "exploring: 1\n",
            "abstract: 1\n",
            "ideas: 1\n",
            "transforms: 1\n",
            "imagination: 1\n",
            "visual: 1\n",
            "expression: 1\n",
            "leaving: 1\n",
            "timeless: 1\n",
            "impact: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Stemming and Lemmaztiation\n",
        "1. Take the tokenized words from Question 1 (after stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply lemmatization using NLTK's WordNetLemmatizer.\n",
        "4. Compare and display results of both techniques."
      ],
      "metadata": {
        "id": "KW4NBNFN2fve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for word in filtered_words:\n",
        "    print(f\"Word: {word}\")\n",
        "    print(f\"Porter Stemmer: {porter.stem(word)}\")\n",
        "    print(f\"Lancaster Stemmer: {lancaster.stem(word)}\")\n",
        "    print(f\"WordNet Lemmatizer: {lemmatizer.lemmatize(word)}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "kaQTzsb-2oBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c982bb-b3ce-4fa2-f630-290e8ffea45b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: Painting\n",
            "Porter Stemmer: paint\n",
            "Lancaster Stemmer: paint\n",
            "WordNet Lemmatizer: Painting\n",
            "\n",
            "Word: art\n",
            "Porter Stemmer: art\n",
            "Lancaster Stemmer: art\n",
            "WordNet Lemmatizer: art\n",
            "\n",
            "Word: form\n",
            "Porter Stemmer: form\n",
            "Lancaster Stemmer: form\n",
            "WordNet Lemmatizer: form\n",
            "\n",
            "Word: brings\n",
            "Porter Stemmer: bring\n",
            "Lancaster Stemmer: bring\n",
            "WordNet Lemmatizer: brings\n",
            "\n",
            "Word: colors\n",
            "Porter Stemmer: color\n",
            "Lancaster Stemmer: col\n",
            "WordNet Lemmatizer: color\n",
            "\n",
            "Word: emotions\n",
            "Porter Stemmer: emot\n",
            "Lancaster Stemmer: emot\n",
            "WordNet Lemmatizer: emotion\n",
            "\n",
            "Word: stories\n",
            "Porter Stemmer: stori\n",
            "Lancaster Stemmer: story\n",
            "WordNet Lemmatizer: story\n",
            "\n",
            "Word: life\n",
            "Porter Stemmer: life\n",
            "Lancaster Stemmer: lif\n",
            "WordNet Lemmatizer: life\n",
            "\n",
            "Word: canvas\n",
            "Porter Stemmer: canva\n",
            "Lancaster Stemmer: canva\n",
            "WordNet Lemmatizer: canvas\n",
            "\n",
            "Word: Whether\n",
            "Porter Stemmer: whether\n",
            "Lancaster Stemmer: wheth\n",
            "WordNet Lemmatizer: Whether\n",
            "\n",
            "Word: capturing\n",
            "Porter Stemmer: captur\n",
            "Lancaster Stemmer: capt\n",
            "WordNet Lemmatizer: capturing\n",
            "\n",
            "Word: beauty\n",
            "Porter Stemmer: beauti\n",
            "Lancaster Stemmer: beauty\n",
            "WordNet Lemmatizer: beauty\n",
            "\n",
            "Word: nature\n",
            "Porter Stemmer: natur\n",
            "Lancaster Stemmer: nat\n",
            "WordNet Lemmatizer: nature\n",
            "\n",
            "Word: exploring\n",
            "Porter Stemmer: explor\n",
            "Lancaster Stemmer: expl\n",
            "WordNet Lemmatizer: exploring\n",
            "\n",
            "Word: abstract\n",
            "Porter Stemmer: abstract\n",
            "Lancaster Stemmer: abstract\n",
            "WordNet Lemmatizer: abstract\n",
            "\n",
            "Word: ideas\n",
            "Porter Stemmer: idea\n",
            "Lancaster Stemmer: idea\n",
            "WordNet Lemmatizer: idea\n",
            "\n",
            "Word: transforms\n",
            "Porter Stemmer: transform\n",
            "Lancaster Stemmer: transform\n",
            "WordNet Lemmatizer: transforms\n",
            "\n",
            "Word: imagination\n",
            "Porter Stemmer: imagin\n",
            "Lancaster Stemmer: imagin\n",
            "WordNet Lemmatizer: imagination\n",
            "\n",
            "Word: visual\n",
            "Porter Stemmer: visual\n",
            "Lancaster Stemmer: vis\n",
            "WordNet Lemmatizer: visual\n",
            "\n",
            "Word: expression\n",
            "Porter Stemmer: express\n",
            "Lancaster Stemmer: express\n",
            "WordNet Lemmatizer: expression\n",
            "\n",
            "Word: leaving\n",
            "Porter Stemmer: leav\n",
            "Lancaster Stemmer: leav\n",
            "WordNet Lemmatizer: leaving\n",
            "\n",
            "Word: timeless\n",
            "Porter Stemmer: timeless\n",
            "Lancaster Stemmer: timeless\n",
            "WordNet Lemmatizer: timeless\n",
            "\n",
            "Word: impact\n",
            "Porter Stemmer: impact\n",
            "Lancaster Stemmer: impact\n",
            "WordNet Lemmatizer: impact\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Regular Expressions and Text Splitting\n",
        "1. Take their original text from Question 1.\n",
        "2. Use regular expressions to:\n",
        "a. Extract all words with more than 5 letters.\n",
        "b. Extract all numbers (if any exist in their text).\n",
        "c. Extract all capitalized words.\n",
        "3. Use text splitting techniques to:\n",
        "a. Split the text into words containing only alphabets (removing digits and special\n",
        "characters).\n",
        "b. Extract words starting with a vowel."
      ],
      "metadata": {
        "id": "otSJLTIM2obS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text5 = \"He paid 42.5 dollars in India for a beautiful Umbrella on 3.14. HTML and USA are acronyms.\"\n",
        "\n",
        "words = re.findall(r'\\b\\w{6,}\\b',text5)\n",
        "numbers = re.findall(r'\\d+(?:\\.\\d+)?',text5)\n",
        "capitalized = re.findall(r'\\b[A-Z][a-z]*\\b',text5)\n",
        "\n",
        "print(words)\n",
        "print(numbers)\n",
        "print(capitalized)\n"
      ],
      "metadata": {
        "id": "sHzXo4g720cH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f20c83-7a86-4b31-f9f8-5951abf2d6c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dollars', 'beautiful', 'Umbrella', 'acronyms']\n",
            "['42.5', '3.14']\n",
            "['He', 'India', 'Umbrella']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"He paid 42.5 dollars in India for a beautiful Umbrella on 3.14. HTML and USA are acronyms.\"\n",
        "\n",
        "words = text.split()\n",
        "alpha_words = [word.strip(\".,?!\") for word in words if word.strip(\".,?!\").isalpha()]\n",
        "\n",
        "vowel_words = [word for word in alpha_words if word[0].lower() in 'aeiou']\n",
        "\n",
        "print(\"Alphabetic words:\", alpha_words)\n",
        "print(\"Words starting with a vowel:\", vowel_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFC92kr19UgO",
        "outputId": "7ec42222-56a8-42f9-e436-417fa521d98c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alphabetic words: ['He', 'paid', 'dollars', 'in', 'India', 'for', 'a', 'beautiful', 'Umbrella', 'on', 'HTML', 'and', 'USA', 'are', 'acronyms']\n",
            "Words starting with a vowel: ['in', 'India', 'a', 'Umbrella', 'on', 'and', 'USA', 'are', 'acronyms']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Custom Tokenization & Regex-based Text Cleaning\n",
        "1. Take original text from Question 1.\n",
        "2. Write a custom tokenization function that:\n",
        "a. Removes punctuation and special symbols, but keeps contractions (e.g.,\n",
        "\"isn't\" should not be split into \"is\" and \"n't\").\n",
        "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token).\n",
        "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as is).\n",
        "\n",
        "3. Use Regex Substitutions (re.sub) to:\n",
        "a. Replace email addresses with '<EMAIL>' placeholder.\n",
        "b. Replace URLs with '<URL>' placeholder.\n",
        "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
        "'<PHONE>' placeholder."
      ],
      "metadata": {
        "id": "lbkBu8_1200P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text3 = \"He isn't aware of the state-of-the-art methods used in the 3.14 simulation. You'll need a well-thought-out plan, or it won't succeed. I paid 2.50 dollars for a top-of-the-line pen, but it didn't work.\"\n",
        "\n",
        "def custom_tokenize(text3):\n",
        "  pattern = r'\\d+\\.\\d+|\\b\\w+(?:-\\w+)+|\\b\\w+(?:\\'\\w+)?|\\b\\w+\\b'\n",
        "  tokens = re.findall(pattern, text3)\n",
        "  return tokens\n",
        "\n",
        "tokens = custom_tokenize(text3)\n",
        "print(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "ISoDv8Tg3L_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02de4fac-7f67-491d-bd1d-a2849eccb834"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', \"isn't\", 'aware', 'of', 'the', 'state-of-the-art', 'methods', 'used', 'in', 'the', '3.14', 'simulation', \"You'll\", 'need', 'a', 'well-thought-out', 'plan', 'or', 'it', \"won't\", 'succeed', 'I', 'paid', '2.50', 'dollars', 'for', 'a', 'top-of-the-line', 'pen', 'but', 'it', \"didn't\", 'work']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text4 =  \"\"\"Contact me at john.doe@example.com or visit https://example.com.\n",
        "Call +91 9876543210 or 123-456-7890 for support.\n",
        "Alternate site: www.test-site.org\"\"\"\n",
        "\n",
        "def sub_sensitive_data(text):\n",
        "  email = r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}\\b'\n",
        "  url = r'\\b(?:https?://|www\\.)\\S+\\b'\n",
        "  phone1 = r'\\b\\d{3}-\\d{3}-\\d{4}\\b'\n",
        "  phone2 = r'\\+?\\d{1,3}\\s\\d{10}\\b'\n",
        "\n",
        "  text = re.sub(email, '<EMAIL>', text)\n",
        "  text = re.sub(url,'<URL>', text)\n",
        "  text = re.sub(phone1, '<PHONE>', text)\n",
        "  text = re.sub(phone2, '<PHONE>' , text)\n",
        "\n",
        "  return text\n",
        "\n",
        "cleaned_text = sub_sensitive_data(text4)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "xlj6-ZJ0SUnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8582e8f-3a38-44cb-c6ac-9e85aa6daf4b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contact me at <EMAIL> or visit <URL>.\n",
            "Call <PHONE> or <PHONE> for support.\n",
            "Alternate site: <URL>\n"
          ]
        }
      ]
    }
  ]
}