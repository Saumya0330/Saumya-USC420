# -*- coding: utf-8 -*-
"""cog_assignment12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YKn_UAXbqVvlY_qqnKqCZA4YDKWo2z34

#Lab Assignemnt-12
Cognitive Computing UCS420


Deploy Chatbot for Healthcare Applications
"""

!pip install transformers datasets torch

from datasets import Dataset

data = {
    "input_text": [
        "Hi", "Hello", "I have a headache", "What should I do if I have a fever?", "I feel dizzy",
        "What should I eat for a cold?", "How to stay healthy?", "What should I do in case of a cut?",
        "How much water should I drink daily?", "Thank you", "Bye"
    ],
    "output_text": [
        "Hello! I am HealthBot. How can I assist you today?", "Hi! Do you have a health-related question?",
        "You should rest, stay hydrated, and take a mild pain reliever if needed.",
        "Drink plenty of fluids and rest. If the fever persists, please consult a doctor.",
        "Sit down, breathe deeply, and drink water. If it continues, seek medical help.",
        "Warm fluids, soups, citrus fruits, and light meals help during a cold.",
        "Eat balanced meals, exercise regularly, stay hydrated, and get enough sleep.",
        "Clean the wound with water, apply antiseptic, and cover it with a clean bandage.",
        "Generally, 2 to 3 liters per day is recommended, but it varies based on your activity.",
        "Youâ€™re welcome! Take care.", "Goodbye! Stay healthy."
    ]
}

dataset = Dataset.from_dict(data)

from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

def preprocess_data(examples):
    inputs = [ex for ex in examples['input_text']]
    outputs = [ex for ex in examples['output_text']]
    input_ids = tokenizer(inputs, truncation=True, padding="max_length", max_length=64, return_tensors="pt").input_ids
    output_ids = tokenizer(outputs, truncation=True, padding="max_length", max_length=64, return_tensors="pt").input_ids
    return {"input_ids": input_ids, "labels": output_ids}

dataset = dataset.map(preprocess_data, batched=True, batch_size=1) #added batch_size
#The following line was causing an error as tokenizer.pad_token == tokenizer.eos_token is a boolean value and not callable
#dataset = dataset.map(preprocess_data, batched=True, tokenizer.pad_token == tokenizer.eos_token)

from transformers import GPT2LMHeadModel, Trainer, TrainingArguments

# Load pre-trained GPT-2 model
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Set up training arguments
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=10,
    save_steps=500,
    save_total_limit=2,
    evaluation_strategy="no",
    weight_decay=0.01
)

# Set up the trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer,
)

# Fine-tune the model
trainer.train()

model.save_pretrained("healthbot_model")
tokenizer.save_pretrained("healthbot_model")

# Load the fine-tuned model and tokenizer
model = GPT2LMHeadModel.from_pretrained("healthbot_model")
tokenizer = GPT2Tokenizer.from_pretrained("healthbot_model")

# Test the chatbot with an example input
input_text = "how to fix fever"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# Generate the response
output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)

# Decode and print the response
response = tokenizer.decode(output[0], skip_special_tokens=True)
print(response)